# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1noATUyL58HXagqiILQ7k85JJrcwSorrT
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import fetch_openml
data = fetch_openml(name='credit-g', version=1, as_frame=True)
df = data.frame
print(df.head())

df.isnull().sum()

plt.figure(figsize=(10, 6))
sns.countplot(x='class', data=df)
plt.title('Class Distribution')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE

# Step 0: Check actual column names
print("Column names in dataset:", df.columns)

# OPTIONAL: Rename the actual column if it's not 'class'
# For example, if it's 'Class' or something else:
df.rename(columns=lambda x: x.strip().lower(), inplace=True)  # normalize column names
if 'class' not in df.columns:
    raise KeyError("Target column 'class' not found. Check column names above.")

# Step 1: Drop rows where target is missing
df = df.dropna(subset=['class'])

# Step 2: Convert labels to numeric
df['class'] = df['class'].map({1: 1, 2: 0, 'good': 1, 'bad': 0})

# Step 3: Drop unmapped values
df = df.dropna(subset=['class'])

# Step 4: Features and labels
X = df.drop('class', axis=1)
y = df['class']

# Step 5: One-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Step 6: Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0
)

# Step 7: Oversample
sm = SMOTE(random_state=0)
X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)

# Step 8: Check result
print("Original training size:", X_train.shape)
print("Resampled training size:", X_train_resampled.shape)

from sklearn.linear_model import LogisticRegression
mlog = LogisticRegression()
mlog.fit(X_train_resampled, y_train_resampled)
accuracy_score(y_test, mlog.predict(X_test))